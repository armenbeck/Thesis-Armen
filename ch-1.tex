\ProvidesFile{ch-1.tex}[2022-06-14 first chapter]

\chapter{DEVELOPMENT  OF  A  COMPUTATIONAL  PLATFORM  FOR  STOCHASTIC  OPTIMIZATION  OF CHEMICAL SPACES}

This chapter is going to be written. I hope

\section{Abstract}
Optimization of chemical systems and processes have been enhanced and enabled by the guidance of algorithms and analytical approaches.  While many methods will systematically investigate how underlying variables govern a given outcome, there is often a substantial number of experiments needed to accurately model these relations.  As chemical systems increase in complexity, inexhaustive processes must propose experiments that efficiently optimize the underlying objective, while ideally avoiding convergence on unsatisfactory local minima.  We have developed the Paddy software package around the Paddy Field Algorithm, a genetic optimization algorithm that propagates parameters  without direct inference of the underlying objective function.  Benchmarked against the Tree of Parzen Estimator, a Bayesian algorithm implemented in the Hyperopt software Library, Paddy displays efficient optimization with lower runtime, and avoidance of early convergence.  Herein we report these findings for the cases of: global optimization of a two-dimensional bimodal distribution, interpolation of an irregular sinusoidal function, hyperparameter optimization of an artificial neural network tasked with classification of solvent for reaction components, and targeted molecule generation via optimization of input vectors for a decoder network.  We anticipate that the facile nature of Paddy will serve to aid in automated experimentation, where minimization of investigative trials and or diversity of suitable solutions is of high priority. 

\section{Introduction}
Optimization is used in all of chemical sciences, including identifying synthetic methodology1–3, chromatography4–6 conditions, calculating transition state geometry7, or selecting materials and drug formulations8–11.  Typically, several parameters or variables need to be optimized that are done either by human chemists using chemical intuition or computational methods to identify suitable conditions12–14.  Enabled by continual advancements in technology, the automated optimization of once manual duties, such as shimming,15 chromatograph peak assignment16 and developing automated bioanalytical workflows have saved countless human hours.  In addition, the optimization methods have also led to the emerging role of artificial intelligence and machine learning in the chemical sciences17,18.  

For many optimization methods in chemistry, the approach is often an iterative process when direct determinate outcomes are impermissible due to computational complexity or incompleteness of theory19–30.  A multitude of Iterative optimization methods have been developed, differing widely in nature and applicability, providing both general and task specific   approaches to optimization problems.   Additionally, iterative optimization methods are often either deterministic or stochastic in nature, presenting a dichotomy in formulation and resulting behavior31.  While deterministic optimization is defined by replicable convergence to a solution, an optimization utilizing a stochastic process generates differing solutions per execution.  Depending on the given optimization problem, deterministic optimization methods are vulnerable to early convergence on nonoptimal local solutions 31–34.  The incorporation of a stochastic element may prove to be greatly beneficial, with an illustrative example being the improved performance and popularity of stochastic gradient decent compared to gradient decent35.  Additionally, iterative optimization methods may be further enhanced by optimizing a population of candidate solutions in parallel, or batches, such as in the case of swarm intelligence36 and other evolutionary or genetic algorithms37.

While not formally belonging to the field of mathematic optimization, machine learning has been used to predict solutions to optimization problems, with a plethora of chemical applications being reported in recent.  Machine learning has been applied for tasks such as retrosynthesis36, reaction condition prediction39–42, catalyst design43,44, drug design45–48, spectral interpretation49–51, retention time prediction52, and molecular simulations53–55, and is becoming ever more prevalent in the chemical sciences.  A significant paradigm emerging is the generative use of artificial neural networks for tasks including inverse design56,57 and property targeted driven generation of molecules45,58–60.  However, despite the celebrated success and developments made using neural networks and other machine learning algorithms for the optimization of chemical problems and entities, genetic algorithms and other ‘simple’ iterative methods continue to provide utility as optimizers59.  Applications of sequential optimization algorithms in conjunction with machine learning have been reported in the chemical literature as of recent, an intuitive synergy lending to the ready employability of various algorithms which don’t require training prior to use62–64.  

Through directed sampling via maximization of a fitness function, sequential optimization algorithms propagate parameters in an attempt to find a solution for a given optimization problem.  For a multitude of these methods, their design can be described as a (meta)heuristic approach to optimization, where a set of rules define how candidates propagate between iterations.  These internal rules vary between the vast number of heuristic approaches, with simulated aneling65, genetic algorithms66, Tabu search65, hill climbing methods66, and particle swarm69 being notable examples.  In contrast, Bayesian methods lend to directed optimization, guided by sequential updates of a probabilistic model and inferring the return on sampling, often via an acquisition function68.  Furthermore, Bayesian optimization methods have also been reported in the chemical literature for the optimization of neural networks39, generative sampling71,72, and as a general purpose optimizer for chemistry73,74.  

Herein we implement the Paddy field algorithm75 as a Python library that includes heuristic methods Click or tap here to enter text.and additional design formulations.  In this work we show the advantages of using Paddy, when compared to Bayesian optimization via the Hyperopt library76, for the test cases of: finding the global maxima of a two-dimensional bimodal distribution, interpolation of an irregular sinusoidal, hyperparameters optimization of a neural network, and targeted molecular generation.  Additionally, Paddy was designed with usability in mind having native recovery features built in and with full documentation and code available on Github (https://github.com/chopralab/paddy).  We hope that the facile and effective optimization displayed by Paddy will encourage others to employ it for various chemical optimization tasks.

\section{Methods}
Herein we describe the initial description of the PFA, Paddy specific implementations, and its extended formulations. 

\subsection{Formulation of the native Paddy Field Algorithm}
The PFA was inspired by the reproductive behavior of plants, namely the effects of soil quality and pollination on propagation, and it iteratively optimizes in a five-phase process (Figure 1).  For a given fitness (objective) function, the output provides numeric fitness scores analogous to soil quality, while spatial distance between function parameters allows for the incorporation of plant density mediated pollination.  The PFA treats individual parameters x=\{x$_{1}$,x$_{2}$,x$_{3}$,...x$_{n}$\} as seeds, which are then referred to as plants post evaluation.  These seeds are evaluated using an objective function and may inherently be or encompassed by a fitness function that provides a single positive fitness score.  Seeds that result in plants of high fitness are selected from the evaluated population (X) to limit ineffectual and excessive evaluations and promote directed propagation towards improved solutions.  The number of neighboring plants and their fitness score determine the number of seeds produced by a plant selected for propagation.  The parameter values for parent plants are then modified by sampling a normal distribution, facilitating the semi-stochastic optimization process of the algorithm.  The five phases of the algorithm are executed and defined in further detail as:
\section*{Sowing}
The algorithm is initiated by randomly sowing seeds across the objective function’s parameter space.  The number of random seeds is a user defined value, allowing one to account for dimensionality of the objective function and numeric scale of parameters by sowing more seeds in proportion to the scale of parameter space.
\section*{Selection}
After evaluating seeds with the fitness function, the resulting fitness values and respective parameters are considered as ‘plants’.  Post evaluation, a threshold operator (H) is applied to select plants, including those from previous iterations, for further propagation.  The threshold operator value is a positive integer, and controls the degree that cost of evaluation is allocated towards global exploration.

The fitness of the plant that is to the degree of the threshold operator, the nth best plant, is denoted as y$_{t}$, and is used in the subsequent phase when calculating new seeds for selected plants (eq 1).  While all plants where y $\geq$ y$_{t}$ are used in subsequent phases, one should note that a fitness of y$_{t}$ will produce zero new seeds.